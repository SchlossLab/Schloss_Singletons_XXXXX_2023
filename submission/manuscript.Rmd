---
bibliography: references.bib
output:
  pdf_document:
    keep_tex: true
csl: mbio.csl
geometry: margin=1.0in
header-includes:
 - \usepackage{booktabs}
 - \usepackage{longtable}
 - \usepackage{array}
 - \usepackage{multirow}
 - \usepackage{wrapfig}
 - \usepackage{float}
 - \usepackage{colortbl}
 - \usepackage{pdflscape}
 - \usepackage{tabu}
 - \usepackage{threeparttable}
 - \usepackage{threeparttablex}
 - \usepackage[normalem]{ulem}
 - \usepackage{makecell}
 - \usepackage{setspace}
 - \doublespacing
 - \usepackage[left]{lineno}
 - \linenumbers
 - \modulolinenumbers
 - \usepackage{helvet} % Helvetica font
 - \renewcommand*\familydefault{\sfdefault} % Use the sans serif version of the font
 - \usepackage[T1]{fontenc}
---


```{r knitr_settings, tidy=TRUE, eval=TRUE, echo=FALSE, cache=FALSE, warning=FALSE}
options(tidyverse.quiet = TRUE)

library(tidyverse)
suppressPackageStartupMessages(library(glue))
suppressPackageStartupMessages(library(kableExtra))
library(knitr)
suppressPackageStartupMessages(library(here))

opts_chunk$set("tidy" = TRUE)
opts_chunk$set("echo" = FALSE)
opts_chunk$set("eval" = TRUE)
opts_chunk$set("warning" = FALSE)
opts_chunk$set("message" = FALSE)
opts_chunk$set("cache" = FALSE)

inline_hook <- function(x, digits=2){

  if(is.list(x)){
    x <- unlist(x)
  }
  if(is.numeric(x)){
      paste(format(x,big.mark=',', digits=digits, nsmall=digits, scientific=FALSE))
  } else {
      paste(x)
  }
}
knitr::knit_hooks$set(inline=inline_hook)

package_version <- function(package){

  paste(unlist(packageVersion(package)), collapse='.')

}


oxford_comma <- function(x, digits=2) {

	x <- map_chr(x, inline_hook, digits=digits)

	if(length(x) < 2){
		x
	} else if(length(x) == 2){
		paste(x, collapse = " and ")
	} else {
		paste(paste(x[-length(x)], collapse=", "), x[length(x)], sep=", and ")
	}
}
```

# Removal of rare amplicon sequence variants from 16S rRNA gene sequence surveys biases the interpretation of community structure data

\vspace{10mm}

**Running title:** Removal of rare ASVs biases community analyses

\vspace{25mm}

Patrick D. Schloss${^\dagger}$

\vspace{40mm}

$\dagger$ To whom correspondence should be addressed:

\href{mailto:pschloss@umich.edu}{pschloss@umich.edu}

Department of Microbiology and Immunology
University of Michigan
Ann Arbor, MI 48109

\vspace{35mm}

## Research article format

\newpage

<!-- \linenumbers -->

```{r datasets}
table_1 <- read_tsv(here('data/process/study_summary_statistics.tsv'))

n_sample_range <- table_1 %>%
	pull(n_samples) %>%
	range() %>% as.integer()

n_med_sequence_range <- table_1 %>%
	pull(median) %>%
	range() %>% as.integer()

low_fold_samples <- table_1 %>%
	filter(fold_difference < 2) %>%
	pull(directory)

high_fold_range <- table_1 %>%
	filter(fold_difference >= 2) %>%
	pull(fold_difference) %>%
	range() %>%
	round(digits=1)
```

## Abstract

Methods for remediating PCR and sequencing artifacts in 16S rRNA gene sequence collections are in continuous development and have significant ramifications on the inferences that can be drawn. A common approach is to remove rare amplcon sequence variants (ASVs) from datasets. But, the definition of rarity is generally selected without regard for the number of sequences in the samples or the variation in sequencing depth across samples within a study. I analyzed the impact of removing rare ASVs on metrics of alpha and beta diversity using samples collected across `r nrow(table_1)` published datasets. Removal of rare ASVs significantly decreased the number of ASVs and operational taxonomic units as well as their diversity. Furthermore, their removal increased the variation in community structure between samples. When simulating a known effect size, removal of rare ASVs reduced the power to detect the effect relative to not removing rare ASVs. Removal of rare ASVs did not affect the false detection rate when samples were randomized to simulate a null model. However, the false detection rate increased when rare ASVs were removed using a null distribution and assignment of samples to simulated treatment groups according to their sequencing depth. The false detection rate did not vary when rare ASVs were retained. This analysis demonstrates the problems inherent in removing rare ASVs. Researchers are encouraged to retain rare ASVs, to select approaches that minimize PCR and sequencing artifacts, and to use rarefaction to control for uneven sequencing effort.

\newpage

## Importance

Removing rare amplicon sequence variants (ASVs) from 16S rRNA gene sequence collections is an approach that has grown in popularity for limiting PCR and sequencing artifacts. Yet, it is unclear what impact an abundance-based filter has on downstream analyses. To investigate the effects of removing rare ASVs, I analyzed the community distributions found in the samples of `r nrow(table_1)` published datasets. Analysis of these data and simulations based on them showed that removal of rare ASVs distorts the representation of microbial communities. This has the effect of artificially making it more difficult to detect differences between treatment groups. Also of concern was the observation that if sequencing depth is confounded with the treatment, then the probability of falsely detecting a difference between the treatment groups increased with the removal of rare ASVs. The practice of removing rare ASVs should stop, lest researcher adversely affect the interpretation of their data.

\newpage

## Introduction

16S rRNA gene sequencing is a mainstay of microbial community analysis [@Knight2018]. Two elements that are held in tension in the analysis of 16S rRNA gene sequence data are how to adequately remove PCR and sequencing artifacts while decreasing the granularity of the taxonomic level that is used in the analysis. When coarse taxonomic levels (e.g. genus level) are used, the effects of artifacts are minimized since the genetic breadth of the level is wider than the diversity of artifacts. Conversely, with fine taxonomic levels (e.g. amplicon sequence variants; ASVs) the effects of artifacts are significant since each artifact may represent a new ASV.

Numerous studies have attempted to address the problem of removing or "denoising" artifacts from data generated using Illumina's MiSeq platform. In one approach, paired sequence reads are aligned and any discrepencies between the reads are resolved based on the difference in quality score for the position in question [@Kozich2013; @Edgar2016; @Masella2012]; quality scores are also used to curate single reads [@Bokulich2013]. In addition, a polishing step is often used to identify ASVs based on the frequency and similarity of sequences [@Kozich2013; @Edgar2016; @Amir2017]. In a second approach, the quality scores and types of errors are modelled to cluster sequence reads directly into ASVs [@Callahan2016]. Regardless of the approach, many pipelines advocate for abundance-based screening where rare sequences are removed from each dataset prior to outputting the sequence data as ASVs [@Edgar2016; @Amir2017; @Callahan2016]. Popular algorithms employ abundance-based filtering of sequences that appear one [i.e. singletons, DADA2, @Callahan2016], four [UNOISE2, @Edgar2016], or ten [Deblur, @Amir2017] or fewer times; these pipelines also vary in whether the minimum abundance threshold should be applied to individual samples [@Callahan2016; @Amir2017] or the pool of counts from across the samples [@Edgar2016]. Beyond these abundance-based approaches, researchers will also often employ a prevalence-based filter. For example, Nearing et al. [@Nearing2022] used Deblur to produce ASVs [@Amir2017], which removes any sequences that appear 10 or fewer times in each sample and required that an ASV appear in 10% of samples seemingly regardless of their abundance in those samples. A notable exception, the mothur-based pipeline discourages the practice of removing rare sequences [@Kozich2013]. Once sequences are assigned to ASVs, ASVs are often analyzed as their own taxonomic unit or clustered to generate operational taxonomic units (OTUs) or phylotypes.

The abundance and prevalence-based screening approach assumes that rare ASVs are more likely to be artifacts than more abundant ASVs. Sequencing of mock communities confirms that artifacts tend to be rare [@Kozich2013; @Bokulich2013]. Proponents of abundance-based screening point to their ability to obtain the correct number of ASVs, OTUs, or phylotypes with data generated from sequencing mock communities when rare ASVs are removed [@Bokulich2013]. However, this approach risks overfitting the curation pipeline to data generated from a phylogenetically simple community with an atypical community distribution that is often sequenced to a coverage that is not achieved with biological samples. It is necessary to think more deeply about the practice of abundance-based screening and question whether all rare or patchily-distributed sequences are artifacts. Rarity and patchiness are important ecological concept that appear to make microbial ecologists uncomfortable.

The minimum abundance thresholds that have been proscribed were largely developed and applied without regard for the number of sequences generated from each sample. These recommendations appear to assume that the number of reads per sample is consistent within and between studies. However, it is common that the number of sequences generated from each sample may vary by two or three orders of magnitude (e.g. Table 1 and Figure S1). An ASV that appears once in a sample with 2,000 sequences is more trustworthy than an ASV that appears once in a sample with 100,000 sequences since it has a 50-fold higher relative abundance. But, according to the pipeline recommendations, they are treated as being equally trustworthy. One exception to the use of threshold counts was proposed by Bokulich et al. [@Bokulich2013] who suggested a 0.005% relative abundance threshold; however, they cautioned that this threshold was dependent on the quality of the data and should be reassessed with mock community data. Rather than removing rare or patchy ASVs, the approach taken by the mothur pipeline applies the classical ecological approach of rarefaction. Each sample is rarefied to the same sequencing depth so that the number of artifacts that appears in each sample is controlled and problems inherent in using relative abundance, scaling, and other normalization procedures are avoided [@Schloss2023rare; @Schloss2023wnwn].

Experience sequencing biological samples also demonstrates that there are *bona fide* ASVs that may have an abundance below the proscribed threshold. For example, the abundance of an ASV may be below the threshold in some samples or time points and above the threshold in others. For example, my research group frequently encounters cases where it is possible to culture *Clostridiodes difficile* from human and mouse fecal samples, but not find their 16S rRNA gene sequences or to find their sequences early in colonization but not once the organism has been cleared [e.g. @Lesniak2022; @Schubert2014]; naturally, we only find their sequences when the plating density is the highest. This seeming patchiness is a product of their ecology and not an artifact. Rarity, both in terms of abundance and prevalence, is an important ecological concept [@Reid2011]. Removing rare ASVs likely hinders one's to ability to make biological inferences about the dynamics and nature of the populations that rare ASVs represent. Furthermore, removing ASVs whose abundances are below the proscribed threshold also potentially biases the community structure of the samples.

In the current study, I used sequence data from `r nrow(table_1)` published studies to investigate the nature of rare ASVs (i.e. those that appear 10 or fewer times) and the effect of removing them on downstream analysis of microbial communities. The analysis was also performed using traditional OTUs, where ASVs subjected to abundance-based screening were clustered such that the ASVs within an OTU were no more than 3% different from each other. The results reject the assumptions built into abundance-based screening and highlight the problems inherent in removing rare ASVs.


## Results

**Datasets.** I collected `r nrow(table_1)` publicly available datasets that used the Illumina MiSeq platform to sequence the V4 region of the 16S rRNA gene from a variety of environments (Table 1; Figure S1). After removing poor quality and chimeric ASVs and samples that had uncharacteristically low number of sequences for the dataset, these datasets included between `r oxford_comma(n_sample_range)` samples. The median number of sequences for each dataset ranged between `r oxford_comma(n_med_sequence_range)`. Strikingly, aside from the `r oxford_comma(low_fold_samples)` datasets, which had a relatively small number of samples, the difference between the sample with the fewest sequences and the sample with the most sequences for each dataset varied by between `r oxford_comma(high_fold_range, digits=1)`-fold.


```{r results_a}

frac_lost <- read_tsv(here('data/process/sequence_loss_table_raw.tsv')) %>%
	filter(freq_removed == 1) %>%
	group_by(dataset) %>%
	summarize(median=median(fraction_lost))

fraction_singletons_range <- frac_lost %>% arrange(median) %>% slice(1, nrow(.))
percent_singletons_range_vals <- fraction_singletons_range %>% pull(median) * 100
percent_singletons_range_names <- fraction_singletons_range %>% pull(dataset)


cor_n <- read_tsv(here('data/process/sequence_loss_table_cor.tsv')) %>%
 	filter(freq_removed == 1) %>%
	select(dataset, cor_n_estimate, cor_n_p.value) %>%
	mutate(sig = cor_n_p.value < 0.05)

cor_n_nosig <- cor_n %>% filter(!sig) %>% pull(dataset)

cor_n_sig <- cor_n %>%
	filter(sig) %>%
	select(dataset, cor_n_estimate) %>%
	arrange(desc(cor_n_estimate)) %>%
	slice(1, nrow(.))

cor_n_sig_values <- cor_n_sig %>% pull(cor_n_estimate)
cor_n_sig_names <- cor_n_sig %>% pull(dataset)


frac_coverage <- read_tsv(here('data/process/sequence_coverage_table_raw.tsv')) %>%
	filter(freq_removed == 1) %>%
	group_by(dataset) %>%
	summarize(median=median(coverage))

high_coverage <- frac_coverage %>% filter(median > 0.5) %>% pull(dataset)

coverage_n <- read_tsv(here('data/process/sequence_coverage_table_cor.tsv')) %>%
	filter(freq_removed == 1) %>%
	select(dataset, cor_n_estimate, cor_n_p.value) %>%
	mutate(sig = cor_n_p.value < 0.05)

coverage_n_nosig <- coverage_n %>% filter(!sig) %>% pull(dataset)

coverage_n_sig <- coverage_n %>%
	filter(sig) %>%
	select(dataset, cor_n_estimate) %>%
	arrange(desc(cor_n_estimate)) %>%
	slice(1, nrow(.))

coverage_n_sig_values <- coverage_n_sig %>% pull(cor_n_estimate)
coverage_n_sig_names <- coverage_n_sig %>% pull(dataset)
```

**The nature of singletons.** Removal of rare ASVs is commonly justified as a method of removing ASVs that are artifacts. The median percentage of sequences that were discarded when singleton ASVs were removed from each dataset varied between `r oxford_comma(percent_singletons_range_vals)`% (`r oxford_comma(percent_singletons_range_names)`). If these singleton ASVs were artifacts, then one would expect the number of singleton ASVs to accumulate with sequencing depth. Contrary to this expectation, with the exception of the samples from the `r oxford_comma(cor_n_nosig)` datasets whose Spearman correlation was not significantly different from 0.00 (P>0.05), the fraction of singleton ASVs in samples was negatively correlated with the number of sequences in each sample with a range between `r oxford_comma(cor_n_sig_values)` (`r oxford_comma(cor_n_sig_names)`) (Figures 1A and S2). This showed that with additional sequencing, the probability of seeing singleton ASVs in multiple samples was greater than the probability of generating an artifact. This suggests that the singleton ASVs are not as likely to be artifacts as previously thought. Furthermore, if singleton ASVs were artifacts, then one would not expect to find them in other samples from the same dataset. In fact, singleton ASVs from samples with fewer sequences were often found in samples with more sequences. At least 50% of the singleton ASVs found in the samples from the `r oxford_comma(high_coverage)` datasets were found in another sample from the same dataset (Figure 1B). Considering the likelihood of finding an ASV duplicated in another sample is confounded by the number of samples and inter-sample diversity, the high coverage of singleton ASVs in these datasets was remarkable. The correlation between the number of sequences in a sample and the fraction of that sample's singleton ASVs that were covered by another sample in the dataset was significant and negative for `r nrow(coverage_n) - length(coverage_n_nosig)` of the datasets ranging between `r oxford_comma(coverage_n_sig_values)` for the `r oxford_comma(coverage_n_sig_names)` datasets, respectively ((Figures 1C and S3)). The negative correlation indicated that the singleton ASVs in the smaller samples were more likely to be covered by ASVs in the larger samples. Among the three datasets without a significant correlation (Spearman correlation, P>0.05), the marine and soil datasets had the fewest samples in our collection and the stream dataset already had a high level of coverage regardless of the number of sequences. Contrary to the common motivation for removing rare ASVs, these results indicate that this practice disproportionately impacts samples with fewer sequences and likely removes more non-artifact ASVs than those that are artifacts.


```{r  results_b}
ointra <- read_tsv(here("data/process/ointra_analysis.tsv"),
		col_types = cols(.default=col_double(),
					dataset=col_character(), method=col_character(), group=col_character())
	) %>%
	group_by(dataset, method, min_class) %>%
	summarize(mean_s_frac = 100*(1-mean(s_yes / s_no)),
		mean_h_frac = 100*(1-mean(h_yes / h_no))) %>%
	ungroup()

asv_s_two <- ointra %>% filter(min_class == 2, method== "pc") %>% arrange(mean_s_frac)	%>% slice(1, nrow(.))

asv_s_eleven <- ointra %>% filter(min_class == 11, method== "pc") %>% arrange(mean_s_frac)	%>% slice(1, nrow(.))

asv_h_two <- ointra %>% filter(min_class == 2, method== "pc") %>% arrange(mean_h_frac)	%>% slice(1, nrow(.))

asv_h_eleven <- ointra %>% filter(min_class == 11, method== "pc") %>% arrange(mean_h_frac)	%>% slice(1, nrow(.))
```

**The impact of removing rare ASVs on the information represented in each sample.** Removing rare ASVs will reduce the richness of ASVs (i.e. the number of ASVs per sample) and increase the relative abundance of the remaining ASVs. To quantify the effect of removing rare ASVs on the information contained within each sample, I varied the minimum abundance threshold to simulate removing ASVs of varying rarity from each sample. The richness of ASVs in each sample decreased by between `r oxford_comma(pull(asv_s_two, mean_s_frac), 1)`% (`r oxford_comma(pull(asv_s_two, dataset))`) when removing those ASVs that only appeared once and by between `r oxford_comma(pull(asv_s_eleven, mean_s_frac), 1)`% (`r oxford_comma(pull(asv_s_eleven, dataset))`) when removing those that appeared ten or fewer times from each sample (Figure 2A). Similarly, the Shannon diversity decreased by between `r oxford_comma(pull(asv_h_two, mean_h_frac), 1)`% (`r oxford_comma(pull(asv_h_two, dataset))`) when removing ASVs that only appeared once and by between `r oxford_comma(pull(asv_h_eleven, mean_h_frac), 1)`% (`r oxford_comma(pull(asv_h_eleven, dataset))`) when removing ASVs that appeared ten or fewer times from each sample (Figure 2B). Next, I assigned the ASVs to OTUs, which were defined as a group of ASVs that were more than 97% similar to each other to assess the impact of removing rare ASVs on higher level taxonomic groupings that are commonly used in microbial ecology studies. Although pooling similar ASVs into OTUs reduced the impact of removing the rare ASVs relative to the ASV-based analysis, the minimum abundance threshold still decreased the richness of OTUs and the diversity decreased relative to the full community (Figure S4AB). In contrast to the richness and diversity measurements, the Kullback–Leibler divergence compares the relative abundance of specific ASVs or OTUs between representations of the community. I calculated the Kullback–Leibler divergence between the full communities and those where rare ASVs were removed. As the threshold for removing ASVs increased, the amount of information lost also increased for both ASVs and OTUs (Figure 2C and Figure S4C). The relative loss of information was generally smaller for OTUs than than it was for ASVs. Removing rare ASVs, regardless of abundance threshold, had profound impacts on the representation of the communities.


**Removing treatment group effects from community data.** One challenge with using the observed ASV and OTU frequency data to investigate the effects of removing rare ASVs is that we do not know the true difference between treatment groups to know whether the false detection rate or statistical power have been altered by their removal. Therefore, I used the observed frequency data to simulate samples where I could specify the true difference between the treatment groups. To minimize the effects of the random number generator seed, I replicated the analysis 100 times. The first simulation I created was a null model where all of the samples in a dataset were drawn from the same sampling distribution, without replacement. When I repeated the analysis investigating the effects of removing rare ASVs on richness, diversity, and information loss the losses were larger than were detected with the observed data (Figure S5). This result was likely due to the reduced inter-sample variation and patchiness in the datasets.


```{r results_c}
r_cov_alpha <- read_tsv(here("data/process/ralpha_analysis.tsv"),
		col_types = cols(.default=col_double(),
					dataset=col_character(), method=col_character())
	) %>% select(dataset, method, prune, starts_with("cv_")) %>%
	filter(prune == 2 | prune ==1) %>%
	pivot_wider(id_cols=c(dataset, method), names_from=prune, values_from=starts_with("cv_")) %>%
	mutate(cv_sobs = cv_sobs_2/cv_sobs_1,
			cv_shannon = cv_shannon_2/cv_shannon_1
		) %>%
	select(dataset, method, cv_sobs, cv_shannon)

pc_cov_alpha <- r_cov_alpha %>% filter(method == "pc")
pc_cov_sobs <- pc_cov_alpha %>% arrange(cv_sobs) %>% slice(1, nrow(.))
pc_cov_shannon <- pc_cov_alpha %>% arrange(cv_shannon) %>% slice(1, nrow(.))


r_cov_bc <- read_tsv(here("data/process/rbeta_analysis.tsv"),
		col_types = cols(.default=col_double(),
					dataset=col_character(), method=col_character())) %>%
		filter(prune == 2 | prune ==1) %>%
		pivot_wider(id_cols=c(dataset, method), names_from=prune, values_from=starts_with("sd_")) %>%
		mutate(cv_diff = `2`/`1`) %>%
		select(dataset, method, cv_diff)

pc_cov_bc <- r_cov_bc %>% filter(method == "pc") %>% arrange(cv_diff) %>% slice(1, nrow(.))
otu_cov_bc <- r_cov_bc %>% filter(method == "otu") %>% arrange(cv_diff) %>% slice(1, nrow(.))
```

**The impact of removing rare ASVs on the information represented between samples.** Considering the loss of richness, diversity, and information when a community has its rarest ASVs removed, it seemed likely that the relationship between communities would also be altered. To assess the impact of removing rare ASVs on measures of alpha diversity between samples I used the null community distribution and calculated the coefficients of variation (COVs, i.e. the standard deviation divided by the mean) for richness and diversity for each study at multiple abundance thresholds. I expected COVs to increase with the removal of rare ASVs because their removal would increase the dissimilarity between the samples. The COVs for the richness of ASVs across the studies after removing singletons were between `r oxford_comma(pull(pc_cov_sobs, cv_sobs), 1)`-times larger than they were without removing singleton ASVs (`r oxford_comma(pull(pc_cov_sobs, dataset))`; Figure 3A). Similarly, the COVs for the diversity of ASVs were between `r oxford_comma(pull(pc_cov_shannon,cv_shannon), 1)`-times larger when singletons were removed than when they were not removed (`r oxford_comma(pull(pc_cov_shannon, dataset))`; Figure 3B). To assess the impact of removing rare ASVs on measures of beta diversity between samples, I calculated the COVs of the Bray-Curtis distances between samples within the same study at multiple abundance thresholds. The COVs between Bray-Curtis distances within a study when singletons were removed was between `r oxford_comma(pull(pc_cov_bc, cv_diff), 1)`-times larger than when they were not removed (`r oxford_comma(pull(pc_cov_bc, dataset))`; Figure 3C). When ASVs were clustered into OTUs the difference in COVs was less than it was for the ASVs, but qualitatively similar (Figure S6). These results supported my prediction that removing rare ASVs would increase the dissimilarity between samples. This increased variation in the data could have a significant impact on the statistical power to detect differences between treatment groups.


```{r results_d}
b_alpha <- read_tsv(here("data/process/bffect_alpha_analysis.tsv"),
		col_types = cols(.default=col_double(),
					dataset=col_character(), method=col_character(), metric=col_character())) %>%
	filter(metric != "invsimpson", dataset != "marine") %>%
	filter(prune == 2 | prune ==1) %>%
	pivot_wider(id_cols=c(dataset, metric, method), names_from=c(prune), values_from=frac_sig) %>%
	mutate(pp_drop = `1` - `2`,
		percentage_drop = 100*(1-`2` / `1`)) %>%
	arrange(percentage_drop)

b_base_pc_sobs <- b_alpha %>% filter(metric == "sobs", method == "pc") %>% arrange(`1`) %>% slice(1, nrow(.)) %>%
	rename('one' = `1`)


b_base_pc_shannon <- b_alpha %>% filter(metric == "shannon", method == "pc") %>% arrange(`1`) %>% slice(1, nrow(.)) %>%
	rename('one' = `1`)

b_pc_sobs <- b_alpha %>% filter(metric == "sobs", method == "pc") %>% slice(1, n())
b_pc_shannon <- b_alpha %>% filter(metric == "shannon", method == "pc") %>% slice(1, n())


b_beta <- read_tsv(here("data/process/bffect_beta_analysis.tsv"),
		col_types = cols(.default=col_double(),
					dataset=col_character(), clustering=col_character())) %>%
	# filter(dataset != "marine") %>%
	filter(prune==11 | prune == 2 | prune ==1) %>%
	pivot_wider(id_cols=c(dataset, clustering), names_from=c(prune), values_from=frac_sig) %>%
	mutate(p_drop_singletons = 100*(1-`2` / `1`),
					p_drop_tens = 100*(1-`11` / `1`)) %>%
	arrange(p_drop_singletons)

b_base_beta <- b_beta %>% filter(clustering == "pc") %>% arrange(`1`) %>%
	rename('one' = `1`)

unfazed_singletons <- b_beta %>% filter(clustering=="pc" & p_drop_singletons <= 0) %>% arrange(dataset) %>% pull(dataset)

fazed_singletons <- b_beta %>% filter(clustering=="pc" & p_drop_singletons > 0) %>% arrange(p_drop_singletons) %>% slice(1, nrow(.))

n_unfazed_tentons <- b_beta %>% filter(clustering=="pc" & p_drop_tens <= 10) %>% nrow()
stopifnot(n_unfazed_tentons == 0)

fazed_tentons <- b_beta %>% filter(clustering=="pc" & p_drop_tens > 10) %>% arrange(p_drop_tens) %>% slice(1, nrow(.))
```

**The impact of removing rare ASVs on the ability to detect statistically significant differences between treatment groups.** To test the effect of increased inter-sample variation, I simulated two treatment groups using ASV data from each study. Half of the samples were randomly assigned to the first treatment group, which followed a null model. The other samples were assigned to a second treatment group where 10% of the ASVs in the null model were randomly selected to have their abundance increased  by 5%. To minimize the effects of the choice of the random number generator seed, each dataset was replicated 100 times. I then tested the ability to detect a difference between the two treatment groups using alpha and beta diversity metrics. The fraction of significant tests (i.e., P<0.05) was a measurement of the statistical power to detect the difference between the treatment groups. When considering the differences in richness and diversity, the marine dataset yielded no simulated sets that were statistically significant, which was likely due to the small number of samples in the study (N=7). Among the remaining datasets, the power to detect a difference in the richness of ASVs ranged between `r oxford_comma(pull(b_base_pc_sobs, one), 2)` (`r oxford_comma(pull(b_base_pc_sobs,dataset))`) and between `r oxford_comma(pull(b_base_pc_shannon, one), 2)` (`r oxford_comma(pull(b_base_pc_shannon, dataset))`) to detect a difference in diversity when using a Wilcoxon test (Figure 4A). When singleton ASVs were removed, the power to detect a difference in the richness of ASVs dropped by between `r oxford_comma(pull(b_pc_sobs, percentage_drop), 1)`% (`r oxford_comma(pull(b_pc_sobs, dataset))`) and by between `r oxford_comma(pull(b_pc_shannon, percentage_drop), 1)`% for their diversity(`r oxford_comma(pull(b_pc_shannon, dataset))`; Figure 4B). The effect of removing rare ASVs on the richness of OTUs and their diversity was similar (Figure S7AB). I used the Bray-Curtis dissimilarity index to compare the simulated communities within each dataset and calculated the power to detect differences between the two simulated treatment groups using the analysis of molecular variance (Figure 4C and S7C). Without removing rare sequences, the power to detect a difference between the two simulated treatment groups varied between `r oxford_comma(pull(b_base_beta, one) %>% range(), 2)` (`r slice(b_base_beta, 1) %>% pull(dataset)` and `r oxford_comma(slice_max(b_base_beta, order_by = one) %>% pull(dataset))`). Aside from the `r oxford_comma(unfazed_singletons)` datasets, the power to detect differences dropped by between `r oxford_comma(pull(fazed_singletons,p_drop_singletons), 1)`% (`r oxford_comma(pull(fazed_singletons, dataset))`) when singletons were removed. However, when ASVs that occurred 10 or fewer times were removed from each sample, the power to detect differences dropped by `r oxford_comma(pull(fazed_tentons, p_drop_tens), 1)`% (`r oxford_comma(pull(fazed_tentons, dataset))`); similar results were observed when ASVs were clustered into OTUs. Removing rare ASVs reduced the ability to detect simulated treatment effects using metrics commonly used to compare microbial communities.

```{r results_e}
type_one_null_alpha_summary <-
  read_tsv(here("data/process/rffect_alpha_analysis.tsv"),
		col_types = cols(.default=col_double(),
					dataset=col_character(), method=col_character(), metric=col_character())) %>%
	filter(dataset != "marine" & metric != "invsimpson") %>%
  filter((metric == "sobs" | metric == "shannon")) %>%
	group_by(prune, method, metric) %>%
	summarize(p=mean(frac_sig), p_sd = sd(frac_sig), .groups="drop") %>%
	summarize(median = median(p),
						min = min(p),
						max = max(p)
						)

type_one_null_beta_summary <- read_tsv(here("data/process/rffect_beta_analysis.tsv"),
		col_types = cols(.default=col_double(),
					dataset=col_character(), clustering=col_character())) %>%
	filter(dataset != "marine") %>%
	group_by(prune, clustering) %>%
	summarize(p=mean(frac_sig), p_sd = sd(frac_sig), .groups="drop") %>%
	summarize(median = median(p),
						min = min(p),
						max = max(p)
						)

type_one_skew_alpha_summary <-
  read_tsv(here("data/process/sffect_alpha_analysis.tsv"),
		col_types = cols(.default=col_double(),
					dataset=col_character(), method=col_character(), metric=col_character())) %>%
	filter(dataset != "marine" & metric != "invsimpson") %>%
  filter((metric == "sobs" | metric == "shannon")) %>%
  group_by(prune, method, metric) %>%
  summarize(p=mean(frac_sig), p_sd = sd(frac_sig), min_p=min(frac_sig)) %>%
  ungroup()

type_one_skew_beta_summary <- read_tsv(here("data/process/sffect_beta_analysis.tsv"),
		col_types = cols(.default=col_double(),
					dataset=col_character(), clustering=col_character())) %>%
	filter(dataset != "marine") %>%
	group_by(prune, clustering) %>%
	summarize(p=mean(frac_sig), p_sd = sd(frac_sig), min_p=min(frac_sig)) %>%
  ungroup() %>%
  mutate(metric = "braycurtis") %>%
  rename(method = clustering)

min_ave_p <-
  bind_rows(type_one_skew_alpha_summary, type_one_skew_beta_summary) %>%
  filter(prune > 1) %>%
  summarize(min_p = min(p)) %>%
  pull(min_p)
```

**The impact of removing rare ASVs on the probability of falsely detecting a difference between treatment groups.** I next asked whether removing rare ASVs could lead to falsely claiming that a treatment effect had a significant effect on community diversity and structure. First, for each dataset I randomly assigned samples to one of two treatment groups. I then determined the richness and diversity of ASVs and OTUs in each of the samples. Testing at an experiment-wise error rate of 0.05, I expected 5% of the iterations (i.e., ~5 iterations) for each dataset to yield a significant test result. Indeed, there was no evidence that removing rare ASVs resulted in an inflated experiment-wise error rate. The average fraction of significant tests did not meaningfully vary from 0.05 across the minimum abundance threshold, dataset, metric of describing sample alpha-diversity, or whether the abundance of ASVs or OTUs were used (median: `r type_one_null_alpha_summary$median`, minimum: `r type_one_null_alpha_summary$min`, maximum: `r type_one_null_alpha_summary$max`; Figure 5A and S8A). Similarly, the average fraction of significant tests did not meaningfully vary from 0.05 when using analysis of molecular variance to compare communities using Bray-Curtis distances (median: `r type_one_null_beta_summary$median`, minimum: `r type_one_null_beta_summary$min`, maximum: `r type_one_null_beta_summary$max`; Figure 5A and S8A). Second, I again sampled sequences from the null distribution, but assigned samples to one of two treatment groups based on the number of sequences in each sample. The samples with fewer than the median number of sequences for the dataset were assigned to one group and those with more than the median were assigned to the other. This exaggerated bias has been observed in comparisons of the lung and oral microbiota because of the larger number of non-specific amplicons that can be sequenced from lung samples relative to those in the oral cavity leading to a significant difference in sequencing depth between treatment groups [@Morris2013]. When rare sequences were not removed, the fraction of significant tests did not differ from 5% for comparing the richness, their diversity, or Bray-Curtis distances (Figure 5B and S8B). However, when rare taxa of any frequency were removed, the probability of falsely detecing a difference as signifiant increased with the definition of rarity (Figure 5B and S8B). Not including the small marine dataset, the average fraction the average fraction of falsely detecting a difference across datasets when only singletons were removed was `r 100*min_ave_p`%. If there is any relationship between the number of sequences and the treatment group, the risk of falsely rejecting the null hypothesis is inflated when researchers use the strategy of removing rare sequences. The most conservative approach is to not remove low abundance sequences.



## Discussion

Removing rare sequences from 16S rRNA gene sequence data is a common practice that is used as a heuristic to help remove residual PCR artifacts and low quality sequences. In this analysis, I have shown that rare sequences are more common in samples with shallow sequencing than in those with deep sequencing and that rare sequences are frequently observed in multiple samples. These observations suggest that many of the sequences being removed are actually good sequences. Becuase rarity is often defined by a fixed number of observations per sample (e.g. sequences that only appear once in a sample, regardless of the size of the sample), removing rare sequences has a disproportionate impact on samples with fewer sequences. Removing rare sequences resulted in a reduction in the alpha diversity and a pronounced change in the structure of individual samples. The effect was an increase in the differences observed between samples, which made it more difficult to detect differences between treatment groups when differences actually existed. Furthermore, if the number of reads per sample was confounded with the treatment groups, then removing rare sequences increased the probability of falsely detecting a difference between the treatment groups. The practice of removing rare sequences from samples should be stopped.

The practice of removing rare sequences from samples seems to be a response to researchers prioritizing the number of reads and length of sequences over their quality and needing a mechanism to reduce the inflated number of spurious OTUs [@Bokulich2013]. Previous work has shown that assembly of fully overlapping sequence reads results in the lowest sequencing error rates [@Kozich2013]. The studies highlighted in this analysis sequenced the V4 region of the 16S rRNA gene using the Illumina MiSeq sequencing platform with their version 2 chemistry. The resulting data consists of two 250 nt reads that span a region that is about 253 nt long. In contrast, there has been a movement to sequence longer regions with similar chemistry resulting in less overlap between the sequencing reads [@Fadrosh2014; @Holm2019]. Alternatively, others have prioritized increasing the number of sequences per sample by sequencing the V4 region, but with single reads or with reads that are only 150 nt long [@Thompson2017; @McDonald2018]. These practices result in a significantly higher error rate for the resulting assembly since each read has an average per base error rate near 1%. With the approach used here, the error rate of the polished sequences for the V4 region is on the order of 0.01%, but it is at least 10-fold higher when sequencing regions that not fully overlap [@Kozich2013]. Another source of artifacts are chimeras, which are formed during PCR [@Sze2019; @Haas2011]. Although lab-based strategies are available to minimize their formation, most pipelines use bioinformatic approaches such as UCHIME to remove these artifacts [@Edgar2011]. Researchers should prioritize the quality over the quantity and length of their data. For these reasons, this analysis did not use lower quality data generated by alternative methods. 

Even with the best sequencing approaches, PCR artifacts and sequencing errors persist. This may account for some sequences not being observed other samples. Although this lack of inter-sample coverage could have been due to treatment effects and natural variation, it is likely that some portion of the sequences that were unique to samples were artifacts or contained errors. Researchers are encouraged to use rarefaction to control for uneven sampling and the presence of spurious sequences. By rarefying data to a common number of sequences per sample, the number of spurious sequences can be controlled. As shown in the data I presented, which was rarefied to a common number of reads per sample within a dataset, when rare sequences were not removed the power to detect differences was the highest (Figures 4 and S7) and the false discovery rate was the expected 5% (Figures 5 and S8). Although using a relative abundance-based filter has been suggested [@Bokulich2013], relative abundance normalization has been shown inferior to rarefaction for alpha and beta diversity analyses [@Schloss2023wnwn; @Schloss2023rare].

In addition to considerations of how to control for the presence of spurious sequences, researchers also need to be mindful of how to interpret the results of their work. Because every dataset will contain residual sequences that are spurious, measures of richness and diversity should be made on a relative basis. For example, pronouncements that communities from an environment or treatment contain a specific number of taxa are problematic. Instead, we should limit ourselves to indicating that samples from one treatment group has more taxa than another without using absolute values of richness. Furthermore, we must take caution in interpreting rare taxa. Although the data from the studies highlighted here suggest that most rare sequences are not spurious, it is likely that some are. Therefore, researchers must approach rare sequences with more skepticism than more abundant sequences. Researchers should seek out other methods to confirm inferences that they make about rare sequences. This is a standard that should be applied regardless of their abundance [@Schloss2018].

How to curate and interpret rare sequences has been a significant challenge since microbial ecologists transitioned away from Sanger sequencing of samples [@Sogin2006; @Huse2010; @Reeder2009]. Although the extent of the "rare biosphere" is still an open question, it is important to appreciate the importance of rare populations in all communities. Populations can be numerically rare but ubuiquitous or abundant and limited in their geographic range. Alternatively, they can be numerically rare but temporally common or abundant but present infrequently. Removing sequences from any of these settings will limit our ability to study the role of such populations or the processes that drive their patchy distributions that are so common with microbial communities [@Reid2011].



## Materials and Methods

**Data curation and analysis.** To ensure the highest possible data quality, datasets were limited to those where the 500 cycle version 2 MiSeq chemistry was used to sequence the amplicons. The paired 250 nt reads resulted in near complete 2-fold sequencing coverage of every nucleotide in the ca. 253 nt-long region. This region and sequencing platform were selected because previous work has shown that a standard data analysis pipeline in mothur results in a sequencing error rate below 0.02% [@Kozich2013]. All sequence data were obtained from the Sequence Read Archive (SRA) and processed using a standard mothur-based sequencing pipeline that resulted in ASVs as generated by the pre.cluster algorithm using a threshold of 2 nt [@Kozich2013; @Schloss2009]. ASVs were assigned to OTUs using a 3% distance threshold using mothur's cluster function with the OptiClust algorithm [@Westcott2017]. To minimize the effects of uneven sampling effort, samples were rarefied to the number of sequences in the smallest sample for each dataset. Because metrics of alpha diversity did not consistently follow a normal distribution, I used the non-parametric Wilcoxon rank test as implemented in R. For comparisons of Bray-Curtis distances, the amova function within mothur was used, which implements the analysis of molecular variance algorithm [@Anderson2001].

Richness
Shannon
Kullback–Leibler divergence
Bray-Curtis
rarefaction

Generated community wiht singletons and that was used for all filtering

**Null model.**
I generated null distributions for each study by randomizing, without replacement, the number of times each ASV was observed in each sample such that the total number of sequences in each sample and the total number of times each ASV was observed across all samples in the study was the same as was originally observed. This effectively made every community in a study a statistical sample of the study-wide composite community distribution. For example, after this procedure, each of the 490 samples from the human dataset would be expected to have the same richness and diversity of ASVs and one would not expect to find treatment-based effects between the samples. Because of the risk of bias if only one representation of the null distribution was generated, I generated 100 randomized datasets for each study. 

**Other model.**
randomly assigned samples to one of two treatment groups. In the first treatment group, samples were regenerated by randomly sampling ASVs from the null distribution as described above such that each samples had the same number of sequences as it did in the original study. For the second treatment group, I randomly selected 10% of the ASVs in the pooled study distribution to increase their abundance by 5%. I randomly generated 100 simulated sets of treatment groups and samples. 

**Reproducibility.** All analyses were performed using mothur (version 1.44.1) and R (version `r paste(R.version$major, R.version$minor, sep='.')`) with the tidyverse (version `r package_version("tidyverse")`), broom (version `r package_version("broom")`), data.table (version `r package_version("data.table")`), and cowplot (version `r package_version("cowplot")`) packages. All of the code that was used in this analysis as well as the Makefile for running the analysis are available at https://github.com/SchlossLab/Schloss_Singletons_XXXXX_2019 along with the complete version history of the project.


## Acknowledgements

I endebted to the researchers who developed the 12 datasets used in this study for depositing their sequence data into the Sequence Read Archive. This work was supported in part by funding from the National Institutes of Health (U01AI124255, P30DK034933, R01CA215574).


\newpage

## References

<div id="refs"></div>


\newpage

**Table 1. Summary of studies used in the analysis.** For all studies, the number of sequences used from each study was rarefied to the smallest sample size. A graphical represenation of the distribution of sample sizes for each study and the samples that were removed from each study are provided in Figure S1.

\small

```{r results_f}
table_1 %>%
  arrange(nice_name) %>%
  mutate(
    nice_name = glue("{nice_name} {reference}"),
    total_seqs=format(total_seqs, big.mark=",", trim=TRUE),
    median=format(as.integer(median), big.mark=",", trim=TRUE),
    min=format(min, big.mark=",", trim=TRUE),
    max=format(max, big.mark=",", trim=TRUE),
    range = glue("{min}-{max}"),
    n_samples = n_samples,
    fold_difference = round(fold_difference, 1)) %>%
  select(nice_name, n_samples, total_seqs, median, range, fold_difference, sra_study) %>%
  kableExtra::kable(format="markdown", booktabs=TRUE, escape=F, align="lrrrrr",
    linesep="",
		col.names = linebreak(
      c(
        "\\textbf{Study (Ref)}",
        "\\textbf{Samples}",
        "\\textbf{Total}\n\\textbf{sequences}",
        "\\textbf{Median}\n\\textbf{sequences}",
        "\\textbf{Range of}\n\\textbf{sequences}",
        "\\textbf{Fold-difference}\n\\textbf{between largest}\n\\textbf{and smallest sample}",
        "\\textbf{SRA study}\n\\textbf{accession}"
      ),
      align="c")
	)
```
\normalsize

\newpage

**Figure 1. Singletons are more common in samples with fewer seqeunces and tend to be shared with samples having more sequences.** For each of the 12 datasets, Spearman correlation coefficients were calcualted between the number of sequences in each sample and the number of singletons in the sample (A) and the fraction of its singletons that were shared with another sample (C). Those correlations that were not statisically significant had a P-value greater than 0.05. The faction of singletons shared across samples (B) were calculated for each dataset. The median value is shown with a solid circle and the 95% confidence interval is indicated by the solid line. The raw data for A and C are presented in Figures S2 and S3, respectively.

**Figure 2. Removing rare sequences from samples alters their representation of alpha-diversity using amplicon sequence variants (ASVs).** The average difference in the richness (A), Shannon diversity (B), and Kullback-Leiber divergence (C) for each sample within a dataset was calculated between the original community structures relative to applying different minimum abundance thresholds.

**Figure 3. Removing rare sequences from samples increases the inter-sample variation for amplicon sequence variants (ASVs).** The coefficient of variation in richness (A), Shannon diversity (B), and Bray-Curtis distances (C) for each dataset was calculated using the null distributed samples for each dataset with varying minimum abundance thresholds.

**Figure 4. Removing rare sequences from samples reduces the statistical power to detect differences between empirically generated treatment groups when using amplicon sequence variants (ASVs).** The fraction of significant tests comparing the richness (A) and Shannon diversity (B) using a Wilcoxon test and Bray-Curtis distances (C) using analysis of molecular variance for each dataset was calculated using empirically generated treatment groups containing equal numbers of samples for each dataset with varying minimum abundance thresholds. For each dataset and minimum abundance threshold, 100 randomizations were peformed.

**Figure 5. Removing rare sequences does not impact the false detection rate unless the number of sequences per sample is confounded with the treatment groups when using amplicon sequence variants (ASVs).** The fraction of significant tests comparing the richness and Shannon diversity using a Wilcoxon test and Bray-Curtis distances using analysis of molecular variance for each dataset was calculated. Empirically generated treatment groups were generated containing equal numbers of samples where the samples represented a null distribution. In one simulation the samples were randomly assigned to a treatment group (A) and in the other the samples were assigned based on the number of sequences in each sample (B). For each dataset and minimum abundance threshold, 100 randomizations were peformed.

\newpage

**Figure S1. Distribution of the number of sequences per sample in the 12 datasets included in this study.** A different minimum number of sequences per sample threshold was applied to each dataset based on identifying natural breaks in the distribution of the number of sequences per sample.

**Figure S2. Singletons are more common in samples with fewer seqeunces.** Spearman correlation coefficients ($\rho$) were calcualted between the total number of sequences and the number of singletons in the sample. The Spearman correlation coefficients for the Marine and Sediment datasets were not significantly different from 0.00 (P>0.05). The blue line indicates the linear trend in the data. These data are summarized in Figure 1A.

**Figure S3. Singletons tend to be shared with samples having more sequences.** Spearman correlation coefficients ($\rho$) were calcualted between the total number of sequences and the number of singletons in a sample that were shared with other samples from the same deataset. The Spearman correlation coefficients for the Marine, Soil, and Stream datasets were not significantly different from 0.00 (P>0.05). The blue line indicates the linear trend in the data. These data are summarized in Figure 1C.

**Figure S4. Removing rare sequences from samples alters their representation of alpha-diversity using operational taxonomic units (OTUs).** The average difference in the richness (A), Shannon diversity (B), and Kullback-Leiber divergence (C) for each sample within a dataset was calculated between the original community structures relative to applying different minimum abundance thresholds.

**Figure S5. Removing rare sequences from samples alters their representation of alpha-diversity when regenerating samples using a null distribution for each dataset.** The average difference in the richness, Shannon diversity, and Kullback-Leiber divergence for each sample within a dataset was calculated between the original community structures relative to applying different minimum abundance thresholds. Some values of Kullback-Leiber divergence are missing because undefined values were calculated due to the removal of rare sequences. Data are shown for amplicon sequence variants (ASVs) and operational taxonomic units (OTUs)

**Figure S6. Removing rare sequences from samples increases the inter-sample variation for operational taxonomic units (OTUs).** The coefficient of variation in richness (A), Shannon diversity (B), and Bray-Curtis distances (C) for each dataset was calculated using the null distributed samples for each dataset with varying minimum abundance thresholds.

**Figure S7. Removing rare sequences from samples reduces the statistical power to detect differences between empirically generated treatment groups when using operational taxonomic units (OTUs).** The fraction of significant tests comparing the richness (A) and Shannon diversity (B) using a Wilcoxon test and Bray-Curtis distances (C) using analysis of molecular variance for each dataset was calculated using empirically generated treatment groups containing equal numbers of samples for each dataset with varying minimum abundance thresholds. For each dataset and minimum abundance threshold, 100 randomizations were peformed.

**Figure S8. Removing rare sequences does not impact the false detection rate unless the number of sequences per sample is confounded with the treatment groups when using operational taxonomic units (OTUs).** The fraction of significant tests comparing the richness and Shannon diversity using a Wilcoxon test and Bray-Curtis distances using analysis of molecular variance for each dataset was calculated. Empirically generated treatment groups were generated containing equal numbers of samples where the samples represented a null distribution. In one simulation the samples were randomly assigned to a treatment group (A) and in the other the samples were assigned based on the number of sequences in each sample (B). For each dataset and minimum abundance threshold, 100 randomizations were peformed.
